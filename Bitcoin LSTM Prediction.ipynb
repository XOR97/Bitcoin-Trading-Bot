{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import lstm, etl, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "configs = json.loads(open('configs.json').read())\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for plotting and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(predicted_data, true_data):\n",
    "    fig=plt.figure(figsize=(18, 12), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    plt.plot(predicted_data, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def predict_sequences_multiple(model, data, window_size, prediction_len):\n",
    "    #Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n",
    "    prediction_seqs = []\n",
    "    for i in range(int(len(data)/prediction_len)):\n",
    "        curr_frame = data[i*prediction_len]\n",
    "        predicted = []\n",
    "        for j in range(prediction_len):\n",
    "            predicted.append(model.predict(curr_frame[np.newaxis,:,:])[0,0])\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
    "        prediction_seqs.append(predicted)\n",
    "    return prediction_seqs\n",
    "\n",
    "def plot_results_multiple(predicted_data, true_data, prediction_len):\n",
    "    fig=plt.figure(figsize=(18, 12), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    #Pad the list of predictions to shift it in the graph to it's correct start\n",
    "    for i, data in enumerate(predicted_data):\n",
    "        padding = [None for p in range(i * prediction_len)]\n",
    "        plt.plot(padding + data, label='Prediction')\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "true_values = []\n",
    "def generator_strip_xy(data_gen, true_values):\n",
    "    for x, y in data_gen_test:\n",
    "        true_values += list(y)\n",
    "        yield x\n",
    "    \n",
    "def fit_model_threaded(model, data_gen_train, steps_per_epoch, configs):\n",
    "    \"\"\"thread worker for model fitting - so it doesn't freeze on jupyter notebook\"\"\"\n",
    "    model = lstm.build_network([ncols, 150, 150, 1])\n",
    "    model.fit_generator(\n",
    "        data_gen_train,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=configs['model']['epochs']\n",
    "    )\n",
    "    model.save(configs['model']['filename_model'])\n",
    "    print('> Model Trained! Weights saved in', configs['model']['filename_model'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Data Loader object and create a clean data set file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Creating x & y data files...\n",
      "> Clean datasets created in file `data/clean_data.h5.h5`\n",
      "> Generating clean data from: data/clean_data.h5 with batch_size: 100\n"
     ]
    }
   ],
   "source": [
    "dl = etl.ETL()\n",
    "dl.create_clean_datafile(\n",
    "    filename_in = configs['data']['filename'],\n",
    "    filename_out = configs['data']['filename_clean'],\n",
    "    batch_size = configs['data']['batch_size'],\n",
    "    x_window_size = configs['data']['x_window_size'],\n",
    "    y_window_size = configs['data']['y_window_size'],\n",
    "    y_col = configs['data']['y_predict_column'],\n",
    "    filter_cols = configs['data']['filter_columns'],\n",
    "    normalise = True\n",
    ")\n",
    "\n",
    "print('> Generating clean data from:', configs['data']['filename_clean'], 'with batch_size:', configs['data']['batch_size'])\n",
    "\n",
    "data_gen_train = dl.generate_clean_data(\n",
    "    configs['data']['filename_clean'],\n",
    "    batch_size=configs['data']['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Clean data has 180610 data rows. Training on 144488 rows with 722 steps-per-epoch\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(configs['data']['filename_clean'], 'r') as hf:\n",
    "    nrows = hf['x'].shape[0]\n",
    "    ncols = hf['x'].shape[2]\n",
    "    \n",
    "ntrain = int(configs['data']['train_test_split'] * nrows)\n",
    "steps_per_epoch = int((ntrain / configs['model']['epochs']) / configs['data']['batch_size'])\n",
    "print('> Clean data has', nrows, 'data rows. Training on', ntrain, 'rows with', steps_per_epoch, 'steps-per-epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model generatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Compilation Time :  0.03999972343444824\n",
      "> Compilation Time :  0.03951525688171387\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 930, in _run\n",
      "    allow_operation=False)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2414, in as_graph_element\n",
      "    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2493, in _as_graph_element_locked\n",
      "    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\n",
      "ValueError: Tensor Tensor(\"lstm_1_input:0\", shape=(?, ?, 4), dtype=float32) is not an element of this graph.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-2-d28307911ad4>\", line 45, in fit_model_threaded\n",
      "    epochs=configs['model']['epochs']\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\models.py\", line 1124, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\engine\\training.py\", line 1902, in fit_generator\n",
      "    class_weight=class_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\engine\\training.py\", line 1642, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2269, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 933, in _run\n",
      "    + e.args[0])\n",
      "TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"lstm_1_input:0\", shape=(?, ?, 4), dtype=float32) is not an element of this graph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = lstm.build_network([ncols, 150, 150, 1])\n",
    "t = threading.Thread(target=fit_model_threaded, args=[model, data_gen_train, steps_per_epoch, configs])\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Testing model on 36122 data rows with 361 steps\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"lstm_1_input:0\", shape=(?, ?, 4), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    929\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 930\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    931\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2413\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2414\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2492\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"lstm_1_input:0\", shape=(?, ?, 4), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c5abe4a4b27c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m predictions = model.predict_generator(\n\u001b[1;32m     12\u001b[0m     \u001b[0mgenerator_strip_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_gen_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_q_size, workers, pickle_safe, verbose)\u001b[0m\n\u001b[1;32m   1195\u001b[0m                                             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m                                             \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                                             verbose=verbose)\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_q_size, workers, pickle_safe, verbose)\u001b[0m\n\u001b[1;32m   2118\u001b[0m                     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1703\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1706\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    931\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 933\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"lstm_1_input:0\", shape=(?, ?, 4), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "data_gen_test = dl.generate_clean_data(\n",
    "    configs['data']['filename_clean'],\n",
    "    batch_size=configs['data']['batch_size'],\n",
    "    start_index=ntrain\n",
    ")\n",
    "\n",
    "ntest = nrows - ntrain\n",
    "steps_test = int(ntest / configs['data']['batch_size'])\n",
    "print('> Testing model on', ntest, 'data rows with', steps_test, 'steps')\n",
    "\n",
    "predictions = model.predict_generator(\n",
    "    generator_strip_xy(data_gen_test, true_values),\n",
    "    steps=steps_test\n",
    ")\n",
    "\n",
    "#Save our predictions\n",
    "with h5py.File(configs['model']['filename_predictions'], 'w') as hf:\n",
    "    dset_p = hf.create_dataset('predictions', data=predictions)\n",
    "    dset_y = hf.create_dataset('true_values', data=true_values)\n",
    "    \n",
    "plot_results(predictions[:800], true_values[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reload the data-generator\n",
    "data_gen_test = dl.generate_clean_data(\n",
    "    configs['data']['filename_clean'],\n",
    "    batch_size=800,\n",
    "    start_index=ntrain\n",
    ")\n",
    "data_x, true_values = next(data_gen_test)\n",
    "window_size = 50 #numer of steps to predict into the future\n",
    "\n",
    "#We are going to cheat a bit here and just take the next 400 steps from the testing generator and predict that data in its whole\n",
    "predictions_multiple = predict_sequences_multiple(\n",
    "    model,\n",
    "    data_x,\n",
    "    data_x[0].shape[0],\n",
    "    window_size\n",
    ")\n",
    "\n",
    "plot_results_multiple(predictions_multiple, true_values, window_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
